{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julian Torres\n",
    "# Artificial Intelligence for Robotics \n",
    "---\n",
    "# Problem Set 3\n",
    "--- \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) \n",
    "\n",
    "## 18.1 \n",
    "**Consider the problem faced by an infant learning to speak and understand a language. Explain how this process fits into the general learning model. Describe the percepts and actions of the infant, and the types of learning the infant must do. Describe the subfunctions the infant is trying to learn in terms of inputs and outputs, and available example data.**\n",
    "\n",
    "Agent: The infant\n",
    "Environment: Caregivers, objects, sounds, social context\n",
    "Percepts:\n",
    "Auditory: Speech sounds, tone, rhythm\n",
    "Visual: Mouth movements, gestures, facial expressions\n",
    "Actions:\n",
    "Babbling, mimicking sounds, eventually forming words and sentences\n",
    "\n",
    "**Types of Learning**\n",
    "•\tSupervised Learning: Caregivers correct or reinforce language use\n",
    "•\tUnsupervised Learning: Identifying word boundaries and patterns in speech\n",
    "•\tReinforcement Learning: Using trial and error to get desired responses (e.g. saying “milk” gets milk)\n",
    "\n",
    "**Subfunctions**\n",
    "1.\tPhoneme Recognition – Identifying and categorizing basic units of sound in speech.\n",
    "2.\tWord Segmentation – Detecting boundaries between words in continuous speech.\n",
    "3.\tWord-Meaning Association – Mapping spoken words to objects, actions, or concepts.\n",
    "4.\tSyntax Acquisition – Learning the rules for combining words into grammatically correct sentences.\n",
    "5.\tPragmatic Understanding – Interpreting meaning based on context, tone, and social cues.\n",
    "6.\tSpeech Production – Physically producing sounds, words, and eventually full sentences.\n",
    "7.\tImitation and Mimicry – Reproducing sounds and speech patterns heard from others.\n",
    "\n",
    "## 18.3\n",
    "**Suppose we generate a training set from a decision tree and then apply decision-tree learning to that training set. Is it the case that the learning algorithm will eventually return the correct tree as the training-set size goes to infinity? Why or why not?**\n",
    "\n",
    "Not necessarily — the decision-tree learning algorithm may not return the exact original tree, even as the training set size goes to infinity. Because multiple trees can be consistent with the same data, also possible to have overfitting or pruning.\n",
    "\n",
    "## 18.17\n",
    "**Construct a support vector machine that computes the XOR function. Use values of +1 and –1 (instead of 1 and 0) for both inputs and outputs, so that an example looks like ([−1, 1], 1) or ([−1, −1], −1). Map the input [x1, x2] into a space consisting of x1 and x1 x2. Draw the four input points in this space, and the maximal margin separator. What is the margin? Now draw the separating line back in the original Euclidean input space.**\n",
    "\n",
    "|x₁\t|x₂\t|XOR(x₁, x₂)\n",
    "| --- --- --- \n",
    "|-1\t|-1\t|-1|\n",
    "|-1\t|+1\t|+1|\n",
    "|+1\t|-1\t|+1|\n",
    "|+1\t|+1\t|- | \n",
    "\n",
    "\n",
    "Map input of [x,x] to a new space and compute transformed points\n",
    "x₁\tx₂\tOutput (y)\tφ(x) = [x₁, x₁·x₂]\n",
    "-1\t-1\t-1\t[-1, 1]\n",
    "-1\t+1\t+1\t[-1, -1]\n",
    "+1\t-1\t+1\t[1, -1]\n",
    "+1\t+1\t-1\t[1, 1]\n",
    "\n",
    "\n",
    "In the feature space [x₁, x₁x₂]:\n",
    "\t•\tClass +1:\n",
    "\t•\t[-1, -1]\n",
    "\t•\t[1, -1]\n",
    "\t•\tClass –1:\n",
    "\t•\t[-1, 1]\n",
    "\t•\t[1, 1]\n",
    "\n",
    "          |\n",
    "     -1   |    +1\n",
    "          |\n",
    "  -1------|------1   → x₁\n",
    "          |\n",
    "     +1   |\n",
    "\n",
    "\n",
    "\t•\tThe separating hyperplane is:\n",
    "\n",
    "x₁ \\cdot x₂ = 0 \\quad \\text{(i.e., horizontal line at } x₁x₂ = 0\\text{)}\n",
    "\n",
    "The margin is the distance from the separating line to the nearest points, which are at x₁x₂ = ±1, so:\n",
    "\n",
    "\\text{Margin} = \\frac{2}{\\|w\\|} = \\frac{2}{\\sqrt{0^2 + 1^2}} = 2\n",
    "\n",
    "\n",
    "(We assume the weight vector w = [0, 1], so the margin is 2 units.)\n",
    "\n",
    "**Mapping separater back to original space**\n",
    "We had:\n",
    "\n",
    "\\phi(x) = [x₁, x₁ \\cdot x₂]\n",
    "\n",
    "\n",
    "The separator in mapped space is:\n",
    "\n",
    "x₁ \\cdot x₂ = 0\n",
    "\n",
    "\n",
    "So the decision boundary in original space is:\n",
    "\n",
    "x₁ \\cdot x₂ = 0\n",
    "\\quad \\Rightarrow \\quad x₁ = 0 \\quad \\text{or} \\quad x₂ = 0\n",
    "\n",
    "\n",
    "## 18.19\n",
    "**Construct by hand a neural network that computes the XOR function of two inputs. Make sure to specify what sort of units you are using.**\n",
    "\n",
    "**Neural Network Architecture**\n",
    "\t•\tInput Layer: 2 nodes → x_1, x_2\n",
    "\t•\tHidden Layer: 2 neurons with tanh activation\n",
    "\t•\tOutput Layer: 1 neuron with tanh activation\n",
    "\n",
    "use the activation function:\n",
    "\n",
    "\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\in (-1, 1)\n",
    "\n",
    "This works well with XOR outputs of +1 / –1.\n",
    "\n",
    "**Weights and Biases**\n",
    "define:\n",
    "\n",
    "Hidden Layer\n",
    "\t•\tNeuron h_1: detects if inputs are different\n",
    "\n",
    "h_1 = \\tanh(w_1^{(1)} x_1 + w_2^{(1)} x_2 + b^{(1)})\n",
    "= \\tanh(1 \\cdot x_1 + 1 \\cdot x_2 + 0)\n",
    "= \\tanh(x_1 + x_2)\n",
    "\n",
    "Neuron h_2: detects if inputs are both positive or both negative\n",
    "\n",
    "h_2 = \\tanh(1 \\cdot x_1 - 1 \\cdot x_2 + 0)\n",
    "= \\tanh(x_1 - x_2)\n",
    "\n",
    "Let the output be:\n",
    "\n",
    "y = \\tanh( w_1^{(2)} h_1 + w_2^{(2)} h_2 + b^{(2)} )\n",
    "\n",
    "\n",
    "Choose weights:\n",
    "\t•\tw_1^{(2)} = 1\n",
    "\t•\tw_2^{(2)} = 1\n",
    "\t•\tb^{(2)} = 0\n",
    "\n",
    "So,\n",
    "\n",
    "y = \\tanh(h_1 + h_2)\n",
    "\n",
    "**Test**\n",
    "Try input x = [1, -1]:\n",
    "\t•\th_1 = \\tanh(1 + (-1)) = \\tanh(0) = 0\n",
    "\t•\th_2 = \\tanh(1 - (-1)) = \\tanh(2) \\approx 0.96\n",
    "\t•\ty = \\tanh(0 + 0.96) \\approx 0.74 \\Rightarrow \\text{close to +1}\n",
    "\n",
    "Try input x = [1, 1]:\n",
    "\t•\th_1 = \\tanh(1 + 1) = \\tanh(2) \\approx 0.96\n",
    "\t•\th_2 = \\tanh(1 - 1) = \\tanh(0) = 0\n",
    "\t•\ty = \\tanh(0.96 + 0) \\approx 0.74 \\Rightarrow \\text{should be -1} \n",
    "\n",
    "So we tweak the network a bit to fix signs.\n",
    "\n",
    "**Reiterating:**\n",
    "Hidden layer:\n",
    "\t•\th_1 = \\tanh(x_1 + x_2)\n",
    "\t•\th_2 = \\tanh(-x_1 - x_2)\n",
    "\n",
    "Output:\n",
    "\t•\ty = \\tanh(h_1 + h_2)\n",
    "\n",
    "Now for input [1, 1]:\n",
    "\t•\th_1 = \\tanh(2) ≈ 0.96\n",
    "\t•\th_2 = \\tanh(-2) ≈ -0.96\n",
    "\t•\ty = \\tanh(0.96 + (-0.96)) = \\tanh(0) = 0 \\Rightarrow \\text{fix with bias}\n",
    "\n",
    "Add output bias:\n",
    "\t•\ty = \\tanh(h_1 + h_2 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) \n",
    "\n",
    "1. Will a loan applicant repay their loan on time?\n",
    "\n",
    "This is the main predictive task in the competition – a binary classification problem\n",
    "\t•\tAlgorithms:\n",
    "\t•\tLogistic Regression\n",
    "\t•\tRandom Forest\n",
    "\t•\tNeural Networks \n",
    "\n",
    "2. Which features are the most important in predicting default risk?\n",
    "\n",
    "This addresses feature importance and explainability, helping stakeholders trust the model.\n",
    "\t•\tAlgorithms:\n",
    "\t•\tTree-based models with built-in feature importance (XGBoost, LightGBM)\n",
    "\t•\tSHAP (SHapley Additive exPlanations) values for any model\n",
    "\n",
    "3. Can we segment applicants into distinct risk profiles?\n",
    "\n",
    "This is an unsupervised learning task aimed at clustering customers based on financial behavior.\n",
    "\t•\tAlgorithms:\n",
    "\t•\tK-Means Clustering\n",
    "\t•\tGaussian Mixture Models (GMM)\n",
    "\t\n",
    "4. Can we detect anomalies or fraudulent loan applications?\n",
    "\n",
    "This involves anomaly detection, useful for identifying rare but risky behaviors.\n",
    "\t•\tAlgorithms:\n",
    "\t•\tAutoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Dataset Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# standardize \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "clf = SVC(kernel='linear')  \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# run prediciton and evaluation \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Iris Dataset Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mushroom Dataset Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1058\n",
      "           1       1.00      1.00      1.00       636\n",
      "\n",
      "    accuracy                           1.00      1694\n",
      "   macro avg       1.00      1.00      1.00      1694\n",
      "weighted avg       1.00      1.00      1.00      1694\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "url_to_dataset = 'https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data'\n",
    "\n",
    "columns = [\n",
    "    'class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "    'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape',\n",
    "    'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',\n",
    "    'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color',\n",
    "    'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat'\n",
    "]\n",
    "df = pd.read_csv(url_to_dataset, header=None, names=columns)\n",
    "\n",
    "# handle missing values ('?') \n",
    "df = df[df['stalk-root'] != '?']\n",
    "\n",
    "# categorical variables\n",
    "label_encoders = {}\n",
    "for column in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# separate features and target\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# train & split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train\n",
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Eval\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Mushroom Dataset Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load Mushroom data from UCI\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data'\n",
    "columns = [\n",
    "    'class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "    'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape',\n",
    "    'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',\n",
    "    'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color',\n",
    "    'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat'\n",
    "]\n",
    "df = pd.read_csv(url, header=None, names=columns)\n",
    "\n",
    "# Handle missing values\n",
    "df = df[df['stalk-root'] != '?']\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for column in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Split features and labels\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train ANN using MLPClassifier\n",
    "ann = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "ann.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate ANN\n",
    "y_pred_ann = ann.predict(X_test)\n",
    "print(\"\\nANN (MLPClassifier) Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ann))\n",
    "print(\"ANN Accuracy:\", accuracy_score(y_test, y_pred_ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision of Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM again for fair comparison\n",
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "print(\"\\nSVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             21613 non-null  int64  \n",
      " 1   date           21613 non-null  object \n",
      " 2   price          21613 non-null  float64\n",
      " 3   bedrooms       21613 non-null  int64  \n",
      " 4   bathrooms      21613 non-null  float64\n",
      " 5   sqft_living    21613 non-null  int64  \n",
      " 6   sqft_lot       21613 non-null  int64  \n",
      " 7   floors         21613 non-null  float64\n",
      " 8   waterfront     21613 non-null  int64  \n",
      " 9   view           21613 non-null  int64  \n",
      " 10  condition      21613 non-null  int64  \n",
      " 11  grade          21613 non-null  int64  \n",
      " 12  sqft_above     21613 non-null  int64  \n",
      " 13  sqft_basement  21613 non-null  int64  \n",
      " 14  yr_built       21613 non-null  int64  \n",
      " 15  yr_renovated   21613 non-null  int64  \n",
      " 16  zipcode        21613 non-null  int64  \n",
      " 17  lat            21613 non-null  float64\n",
      " 18  long           21613 non-null  float64\n",
      " 19  sqft_living15  21613 non-null  int64  \n",
      " 20  sqft_lot15     21613 non-null  int64  \n",
      "dtypes: float64(5), int64(15), object(1)\n",
      "memory usage: 3.5+ MB\n",
      "None\n",
      "                 id         price      bedrooms     bathrooms   sqft_living  \\\n",
      "count  2.161300e+04  2.161300e+04  21613.000000  21613.000000  21613.000000   \n",
      "mean   4.580302e+09  5.400881e+05      3.370842      2.114757   2079.899736   \n",
      "std    2.876566e+09  3.671272e+05      0.930062      0.770163    918.440897   \n",
      "min    1.000102e+06  7.500000e+04      0.000000      0.000000    290.000000   \n",
      "25%    2.123049e+09  3.219500e+05      3.000000      1.750000   1427.000000   \n",
      "50%    3.904930e+09  4.500000e+05      3.000000      2.250000   1910.000000   \n",
      "75%    7.308900e+09  6.450000e+05      4.000000      2.500000   2550.000000   \n",
      "max    9.900000e+09  7.700000e+06     33.000000      8.000000  13540.000000   \n",
      "\n",
      "           sqft_lot        floors    waterfront          view     condition  \\\n",
      "count  2.161300e+04  21613.000000  21613.000000  21613.000000  21613.000000   \n",
      "mean   1.510697e+04      1.494309      0.007542      0.234303      3.409430   \n",
      "std    4.142051e+04      0.539989      0.086517      0.766318      0.650743   \n",
      "min    5.200000e+02      1.000000      0.000000      0.000000      1.000000   \n",
      "25%    5.040000e+03      1.000000      0.000000      0.000000      3.000000   \n",
      "50%    7.618000e+03      1.500000      0.000000      0.000000      3.000000   \n",
      "75%    1.068800e+04      2.000000      0.000000      0.000000      4.000000   \n",
      "max    1.651359e+06      3.500000      1.000000      4.000000      5.000000   \n",
      "\n",
      "              grade    sqft_above  sqft_basement      yr_built  yr_renovated  \\\n",
      "count  21613.000000  21613.000000   21613.000000  21613.000000  21613.000000   \n",
      "mean       7.656873   1788.390691     291.509045   1971.005136     84.402258   \n",
      "std        1.175459    828.090978     442.575043     29.373411    401.679240   \n",
      "min        1.000000    290.000000       0.000000   1900.000000      0.000000   \n",
      "25%        7.000000   1190.000000       0.000000   1951.000000      0.000000   \n",
      "50%        7.000000   1560.000000       0.000000   1975.000000      0.000000   \n",
      "75%        8.000000   2210.000000     560.000000   1997.000000      0.000000   \n",
      "max       13.000000   9410.000000    4820.000000   2015.000000   2015.000000   \n",
      "\n",
      "            zipcode           lat          long  sqft_living15     sqft_lot15  \n",
      "count  21613.000000  21613.000000  21613.000000   21613.000000   21613.000000  \n",
      "mean   98077.939805     47.560053   -122.213896    1986.552492   12768.455652  \n",
      "std       53.505026      0.138564      0.140828     685.391304   27304.179631  \n",
      "min    98001.000000     47.155900   -122.519000     399.000000     651.000000  \n",
      "25%    98033.000000     47.471000   -122.328000    1490.000000    5100.000000  \n",
      "50%    98065.000000     47.571800   -122.230000    1840.000000    7620.000000  \n",
      "75%    98118.000000     47.678000   -122.125000    2360.000000   10083.000000  \n",
      "max    98199.000000     47.777600   -121.315000    6210.000000  871200.000000  \n",
      "id               0\n",
      "date             0\n",
      "price            0\n",
      "bedrooms         0\n",
      "bathrooms        0\n",
      "sqft_living      0\n",
      "sqft_lot         0\n",
      "floors           0\n",
      "waterfront       0\n",
      "view             0\n",
      "condition        0\n",
      "grade            0\n",
      "sqft_above       0\n",
      "sqft_basement    0\n",
      "yr_built         0\n",
      "yr_renovated     0\n",
      "zipcode          0\n",
      "lat              0\n",
      "long             0\n",
      "sqft_living15    0\n",
      "sqft_lot15       0\n",
      "dtype: int64\n",
      "Best alpha for Ridge: 69.05513520162316\n",
      "Ridge Regression MSE: 52150372572.299866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliantorres/miniconda3/envs/ai/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LassoCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# lasso regression ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Initialize Lasso regression model with cross-validation to find the best alpha\u001b[39;00m\n\u001b[1;32m     67\u001b[0m lasso \u001b[38;5;241m=\u001b[39m Lasso()\n\u001b[0;32m---> 68\u001b[0m lasso_cv \u001b[38;5;241m=\u001b[39m \u001b[43mLassoCV\u001b[49m(alphas\u001b[38;5;241m=\u001b[39malphas, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m     69\u001b[0m lasso_cv\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Best alpha\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LassoCV' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('kc_house_data.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Convert 'date' to datetime and extract year and month\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "# Drop columns that won't be used\n",
    "df.drop(['id', 'zipcode', 'lat', 'long'], axis=1, inplace=True)\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# One-hot encode categorical variables if any\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ridge regression ------------------------------------------------------------------\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "# Initialize Ridge regression model with cross-validation to find the best alpha\n",
    "ridge = Ridge()\n",
    "alphas = np.logspace(-6, 6, 200)\n",
    "ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best alpha\n",
    "best_alpha_ridge = ridge_cv.alpha_\n",
    "print(f'Best alpha for Ridge: {best_alpha_ridge}')\n",
    "\n",
    "# Train Ridge regression with the best alpha\n",
    "ridge = Ridge(alpha=best_alpha_ridge)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print(f'Ridge Regression MSE: {mse_ridge}')\n",
    "# lasso regression ------------------------------------------------------------------\n",
    "# Initialize Lasso regression model with cross-validation to find the best alpha\n",
    "lasso = Lasso()\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=10000)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best alpha\n",
    "best_alpha_lasso = lasso_cv.alpha_\n",
    "print(f'Best alpha for Lasso: {best_alpha_lasso}')\n",
    "\n",
    "# Train Lasso regression with the best alpha\n",
    "lasso = Lasso(alpha=best_alpha_lasso)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "print(f'Lasso Regression MSE: {mse_lasso}')\n",
    "\n",
    "# coefficient anaysis and visualization\n",
    "# Ridge coefficients\n",
    "ridge_coefs = pd.Series(ridge.coef_, index=X.columns)\n",
    "\n",
    "# Lasso coefficients\n",
    "lasso_coefs = pd.Series(lasso.coef_, index=X.columns)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Ridge coefficients plot\n",
    "plt.subplot(2, 1, 1)\n",
    "ridge_coefs.sort_values().plot(kind='bar')\n",
    "plt.title('Ridge Regression Coefficients')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient Value')\n",
    "\n",
    "# Lasso coefficients plot\n",
    "plt.subplot(2, 1, 2)\n",
    "lasso_coefs[lasso_coefs != 0].sort_values().plot(kind='bar')\n",
    "plt.title('Lasso Regression Coefficients (Non-zero)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
